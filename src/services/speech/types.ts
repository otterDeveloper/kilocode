// kilocode_change - new file: Speech service type definitions

import { ChildProcess } from "child_process"

/**
 * Speech service states
 */
export enum SpeechState {
	IDLE = "idle",
	RECORDING = "recording",
	TRANSCRIBING = "transcribing",
}

/**
 * Streaming configuration
 */
export interface StreamingConfig {
	chunkDurationSeconds?: number // default 3
	overlapDurationSeconds?: number // default 1
	language?: string
	maxChunks?: number
	hotWordEnabled?: boolean // default false - enable auto-send on hot word detection
	hotWordPhrase?: string // default "kilo code send" - phrase to trigger auto-send
}

/**
 * Audio chunk data for streaming
 */
export interface ChunkData {
	chunkId: number
	filePath: string
	startTime: number
	endTime: number
	sequenceNumber: number
}

/**
 * Progressive transcription result
 */
export interface ProgressiveResult {
	chunkId: number
	text: string
	isInterim: boolean
	confidence: number
	totalDuration: number
	sequenceNumber: number
}

/**
 * Volume sample for real-time audio level monitoring
 */
export interface VolumeSample {
	rmsDb: number // RMS level in dB (e.g., -22.4)
	peakDb: number // Peak level in dB (e.g., -3.1)
	linear: number // Normalized 0..1 value from rmsDb
	at: number // Milliseconds since recording start
}

/**
 * Transcription options
 */
export interface TranscriptionOptions {
	language?: string
	model?: string
	responseFormat?: "json" | "text" | "srt" | "verbose_json" | "vtt"
	/**
	 * Optional text to guide the model's style or continue a previous audio segment.
	 * The prompt should match the audio language and can include:
	 * - Custom vocabulary (product names, technical terms, proper nouns)
	 * - Previous transcript text for chunk stitching
	 * - Style guidance (punctuation, casing)
	 *
	 * Limitations:
	 * - Only last ~224 tokens are used
	 * - Works best with natural-looking text
	 * - Cannot force content not in the audio
	 */
	prompt?: string
}

/**
 * Transcription result
 */
export interface TranscriptionResult {
	text: string
	language?: string
	duration?: number
	segments?: Array<{
		id: number
		start: number
		end: number
		text: string
	}>
}

/**
 * Transcription client interface
 */
export interface ITranscriptionClient {
	transcribe(audioPath: string, options?: TranscriptionOptions): Promise<string>
	transcribeBatch(audioPaths: string[], options?: TranscriptionOptions): Promise<string[]>
}

/**
 * Chunk processor events
 */
export interface ChunkProcessorEvents {
	chunkReady: (chunkPath: string) => void
	chunkError: (error: Error, chunkPath?: string) => void
	complete: () => void
}

/**
 * Chunk processor interface
 */
export interface IChunkProcessor {
	startWatching(ffmpegProcess: ChildProcess, outputDir: string): void
	stopWatching(): Promise<void>
	on<K extends keyof ChunkProcessorEvents>(event: K, listener: ChunkProcessorEvents[K]): this
	off<K extends keyof ChunkProcessorEvents>(event: K, listener: ChunkProcessorEvents[K]): this
	emit<K extends keyof ChunkProcessorEvents>(event: K, ...args: Parameters<ChunkProcessorEvents[K]>): boolean
}

/**
 * Streaming manager interface
 */
export interface IStreamingManager {
	configureHotWord(enabled: boolean, phrase?: string): void
	addChunkText(chunkId: number, text: string): string
	checkHotWord(): { detected: boolean; cleanedText: string }
	getSessionText(): string
	getPreviousChunkText(): string
	reset(): void
}
